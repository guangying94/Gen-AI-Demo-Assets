{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Define environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<key>\"\n",
    "os.environ[\"VISION_API_KEY\"] = \"<key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the deployment name\n",
    "deployment_name: str = \"<Azure Open AI Deployment Name>\"\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base: str = \"https://<resource-name>.openai.azure.com\"\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01.\n",
    "# All versions follow the YYYY-MM-DD date structure.\n",
    "openai_api_version: str = \"2023-12-01-preview\"\n",
    "openai_api_key:str = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The base URL for your vision resource endpoint, e.g. \"https://<your-resource-name>.cognitiveservices.azure.com\"\n",
    "vision_api_endpoint: str = \"https://<resource-name>.cognitiveservices.azure.com\"\n",
    "vision_api_key: str = os.getenv(\"VISION_API_KEY\")\n",
    "\n",
    "# Insert your video SAS URL, e.g. https://<your-storage-account-name>.blob.core.windows.net/<your-container-name>/<your-video-name>?<SAS-token>\n",
    "video_SAS_url: str = \"<video url>\"\n",
    "# This index name must be unique and contain no white spaces.\n",
    "# It must start with alphanumeric, can contain hyphens but they must be followed by alphanumeric (no consecutive hyphens or trailing hyphen).\n",
    "# It must be 24 characters or less.\n",
    "video_index_name: str = \"demo-index-2\"\n",
    "# This video ID must be unique\n",
    "video_id: str = \"demo-video-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Define Azure Open AI Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"GPT-4V_DEPLOYMENT_NAME\": deployment_name,\n",
    "    \"OPENAI_API_BASE\": openai_api_base,\n",
    "    \"OPENAI_API_VERSION\": openai_api_version,\n",
    "    \"VISION_API_ENDPOINT\": vision_api_endpoint,\n",
    "}\n",
    "\n",
    "p = Path(\"./config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Define helper functions\n",
    "\n",
    "The following code defines the helper functions that will be used to interact with the Azure Open AI API.\n",
    "List of helper functions:\n",
    "1. create_video_index\n",
    "2. add_video_to_index\n",
    "3. wait_for_ingest_completion\n",
    "4. process_video_indexing (Calling function 1-3 in sequence)\n",
    "5. call_GPT4V_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def create_video_index(vision_api_endpoint: str, vision_api_key: str, index_name: str) -> object:\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\"features\": [{\"name\": \"vision\", \"domain\": \"surveillance\"}, {\"name\": \"speech\"}]}\n",
    "    return requests.put(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "\n",
    "def add_video_to_index(\n",
    "    vision_api_endpoint: str, vision_api_key: str, index_name: str, video_url: str, video_id: str, ingestion_name: str = \"ingestion-01\"\n",
    ") -> object:\n",
    "    url = (\n",
    "        f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}\"\n",
    "        f\"/ingestions/{ingestion_name}?api-version=2023-05-01-preview\"\n",
    "    )\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"videos\": [{\"mode\": \"add\", \"documentId\": video_id, \"documentUrl\": video_url}],\n",
    "        \"generateInsightIntervals\": False,\n",
    "        \"moderation\": False,\n",
    "        \"filterDefectedFrames\": False,\n",
    "        \"includeSpeechTranscrpt\": True,\n",
    "    }\n",
    "    return requests.put(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "\n",
    "def wait_for_ingestion_completion(\n",
    "    vision_api_endpoint: str, vision_api_key: str, index_name: str, max_retries: int = 30\n",
    ") -> bool:\n",
    "    url = (\n",
    "        f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions?api-version=2023-05-01-preview\"\n",
    "    )\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        time.sleep(10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            state_data = response.json()\n",
    "            if state_data[\"value\"][0][\"state\"] == \"Completed\":\n",
    "                print(state_data)\n",
    "                print(\"Ingestion completed.\")\n",
    "                return True\n",
    "            if state_data[\"value\"][0][\"state\"] == \"Failed\":\n",
    "                print(state_data)\n",
    "                print(\"Ingestion failed.\")\n",
    "                return False\n",
    "        retries += 1\n",
    "    return False\n",
    "\n",
    "def process_video_indexing(\n",
    "    vision_api_endpoint: str, vision_api_key: str, video_index_name: str, video_SAS_url: str, video_id: str\n",
    ") -> None:\n",
    "    # Step 1: Create an Index\n",
    "    response = create_video_index(vision_api_endpoint, vision_api_key, video_index_name)\n",
    "    print(response.status_code, response.text)\n",
    "\n",
    "    # Step 2: Add a video file to the index\n",
    "    response = add_video_to_index(vision_api_endpoint, vision_api_key, video_index_name, video_SAS_url, video_id)\n",
    "    print(response.status_code, response.text)\n",
    "\n",
    "    # Step 3: Wait for ingestion to complete\n",
    "    if not wait_for_ingestion_completion(vision_api_endpoint, vision_api_key, video_index_name):\n",
    "        print(\"Ingestion did not complete within the expected time.\")\n",
    "\n",
    "# Define GPT-4 Turbo with Vision API call with video index\n",
    "def call_GPT4V_video(messages: str, vision_api: object, video_index: object) -> object:\n",
    "    # Construct the API request URL\n",
    "    api_url = (\n",
    "        f\"{openai_api_base}/openai/deployments/{deployment_name}\"\n",
    "        f\"/extensions/chat/completions?api-version={openai_api_version}\"\n",
    "    )\n",
    "\n",
    "    # Including the api-key in HTTP headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": openai_api_key,\n",
    "        \"x-ms-useragent\": \"Azure-GPT-4V-video/1.0.0\",\n",
    "    }\n",
    "\n",
    "    # Payload for the request\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"dataSources\": [\n",
    "            {\n",
    "                \"type\": \"AzureComputerVisionVideoIndex\",\n",
    "                \"parameters\": {\n",
    "                    \"computerVisionBaseUrl\": f\"{vision_api.get('endpoint')}/computervision\",\n",
    "                    \"computerVisionApiKey\": vision_api.get(\"key\"),\n",
    "                    \"indexName\": video_index.get(\"video_index_name\"),\n",
    "                    \"videoUrls\": [video_index.get(\"video_SAS_url\")],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"enhancements\": {\"video\": {\"enabled\": True}},\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800,\n",
    "    }\n",
    "\n",
    "    # Send the request and handle the response\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to make the request. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Call APIs\n",
    "First, call the `process_video_indexing` function to index the video. Then, call the `call_GPT4V_video` function to generate the video demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this cell once to create the index\n",
    "process_video_indexing(vision_api_endpoint, vision_api_key, video_index_name, video_SAS_url, video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System messages and user prompt\n",
    "sys_message = \"\"\"\n",
    "You are an AI assistant that understand images and video content.\n",
    "You only response to user based on the images or videos.\n",
    "Reply \"I don't have this information\" if the user asks for anything else. Keep the response concise.\n",
    "\"\"\"\n",
    "user_prompt = \"is camera visible in this video\"\n",
    "\n",
    "# Make sure that the content of type acv_document_id is first in the use content list like in this example.\n",
    "# Otherwise unexpected behavior can happen.\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": sys_message}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"acv_document_id\", \"acv_document_id\": video_id}, {\"type\": \"text\", \"text\": user_prompt}],\n",
    "    },  # Prompt for the user\n",
    "]\n",
    "\n",
    "vision_api_config = {\"endpoint\": vision_api_endpoint, \"key\": vision_api_key}\n",
    "\n",
    "video_config = {\n",
    "    \"video_SAS_url\": video_SAS_url,\n",
    "    \"video_index_name\": video_index_name,\n",
    "}\n",
    "\n",
    "# Call GPT-4 Turbo with Vision API and print the response\n",
    "try:\n",
    "    response = call_GPT4V_video(messages, vision_api=vision_api_config, video_index=video_config)\n",
    "    text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "    for sentence in sentences:  # Print the content of the response\n",
    "        print(sentence)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to call GPT-4 Turbo with Vision API. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] Clean Up & Troubleshooting\n",
    "You may encounter error on duplicated index, ingestion etc. The following helper functions help to list down the indexes and you can delete the index if duplication occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_video_indexes(vision_api_endpoint: str, vision_api_key: str) -> object:\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def list_video_ingestions(vision_api_endpoint: str, vision_api_key: str, index_name: str) -> object:\n",
    "    url = (\n",
    "        f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions?api-version=2023-05-01-preview\"\n",
    "    )\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def delete_video_index(vision_api_endpoint: str, vision_api_key: str, index_name: str) -> object:\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    return requests.delete(url, headers=headers)\n",
    "\n",
    "def search_video_index(vision_api_endpoint: str, vision_api_key: str, index_name: str, query: str) -> object:\n",
    "    url = (\n",
    "        f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}:queryByText?api-version=2023-05-01-preview\"\n",
    "    )\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\"queryText\": query, \"top\": 10, \"skip\": 0, \"dedup\": True, \"dedupMaxDocumentCount\": 5, \"disableMetadataSearch\": False}\n",
    "    return requests.post(url, headers=headers, data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to list all the video indexes\n",
    "get_video_indexes = list_video_indexes(vision_api_endpoint, vision_api_key)\n",
    "video_indexes = get_video_indexes.json()\n",
    "\n",
    "for index in video_indexes[\"value\"]:\n",
    "    print(f\"Index name: {index['name']}\")\n",
    "    get_video_ingestion = list_video_ingestions(vision_api_endpoint, vision_api_key, 'demo-index-2')\n",
    "    for ingestion in get_video_ingestion.json()[\"value\"]:\n",
    "        print(f\"Ingestion name: {ingestion['name']} - {ingestion['state']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete the video index\n",
    "# Replace 'video_index_name' as needed\n",
    "delete_video_index(vision_api_endpoint, vision_api_key, video_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test out the search functionality\n",
    "query = \"is camera visible in this video\"\n",
    "search_result = search_video_index(vision_api_endpoint, vision_api_key, video_index_name, query)\n",
    "print(json.dumps(search_result.json(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
